{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8c5991c-b49c-491a-88e0-1f6303cf4d61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Mounting of s3 bucketz\n",
    "####url = \"s3://<Access_key_ID>:<encoded_Secret_access_key>@test-autoloder-bucket/growth_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa636fb7-47f7-4ab2-8fb0-def5a7b18baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Access_key_ID = \"****************************\"\n",
    "Secret_access_key = \"*************************\"\n",
    "\n",
    "import urllib\n",
    "encoded_Secret_access_key = urllib.parse.quote(Secret_access_key, \"\")\n",
    "\n",
    "\n",
    "path = 'test-autoloder-bucket/growth_data'\n",
    "url = \"s3a://{0}:{1}@{2}\".format(Access_key_ID, encoded_Secret_access_key, path)\n",
    "\n",
    "dbutils.fs.mount(url, '/mnt/growth_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "795bbd7a-4f3a-44d1-a0e3-c45cb55c9fe0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Set variables for your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77d326f8-1f11-40a6-a220-27e8e164de96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "landing_zone = \"/mnt/growth_data\"\n",
    "incoming_data = landing_zone + \"/incoming_data\"\n",
    "checkpoint_path = landing_zone + \"/growth_checkpoint\"\n",
    "Customer_modelled_data = landing_zone + \"/modelled_data/customer\"\n",
    "orders_modelled_data = landing_zone + \"/modelled_data/orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e327289a-d287-4cf9-9e9c-f0c51ee2f9d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Define Schema and readthe files through autoloader\n",
    "###### you can infer schema, column Types and can give shema hints\n",
    "###### .option(\"cloudFiles.inferSchema\", \"true\") \n",
    "###### .option(\"cloudFiles.schemaLocation\", checkpoint_dir) [make sure you should have initial base files to inferschema]\n",
    "###### .option(\"cloudFiles.inferColumnTypes\", checkpoint_dir)\n",
    "###### .option(\"cloudFiles.schemaHints\", \"col_1 int, col_4 string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cffb2489-b10c-41ae-86ef-471e41732edb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, DoubleType\n",
    "from pyspark.sql.functions import input_file_name, current_timestamp\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_fname\", StringType(), True),\n",
    "    StructField(\"customer_lname\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"pincode\", IntegerType(), True),\n",
    "    StructField(\"line_items\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"order_item_id\", IntegerType(), True),\n",
    "            StructField(\"order_item_product_id\", IntegerType(), True),\n",
    "            StructField(\"order_item_quantity\", IntegerType(), True),\n",
    "            StructField(\"order_item_product_price\", DoubleType(), True),\n",
    "            StructField(\"order_item_subtotal\", DoubleType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "# Load the data with the defined schema\n",
    "read_df = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(incoming_data) \\\n",
    "    .withColumn(\"file_name\", input_file_name()) \\\n",
    "    .withColumn(\"time_of_load\", current_timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9657b2d1-551a-4b3a-bdf8-64f0543b6b05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Basic transformations to refine your nested file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58afc192-18ca-4eb9-af76-9d2bd05d4645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# explode your nested column\n",
    "df = read_df.withColumn(\"new_column\", explode(\"line_items\")).drop(\"line_items\")\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "\n",
    "# flatten your dataframe\n",
    "new_1 = spark.sql(\"\"\" select\n",
    "        order_id,\n",
    "        customer_id,\n",
    "        customer_fname, \n",
    "        customer_lname, \n",
    "        city,\n",
    "        state,\n",
    "        pincode,\n",
    "        new_column.order_item_id as order_item_id,\n",
    "        new_column.order_item_product_id as order_item_product_id,\n",
    "        new_column.order_item_quantity as order_item_quantity,\n",
    "        new_column.order_item_product_price as order_item_product_price,\n",
    "        new_column.order_item_subtotal as order_item_subtotal,\n",
    "        file_name,\n",
    "        time_of_load\n",
    "        from data\n",
    "\"\"\")\n",
    "\n",
    "new_1.createOrReplaceTempView(\"final_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "291107e5-ea5f-49be-91ce-6d03e65c388d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##5. Lets go for DQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74476771-29a7-46c4-b25e-437755bbaaeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. TOP CUSTOMERS WHO SPENT TOTAL AMOUNT \n",
    "\n",
    "total_amount_by_customer = spark.sql(\"\"\"SELECT customer_id, customer_fname, customer_lname, round(SUM(order_item_subtotal), 4) AS TOTAL_AMOUNT_SPENT\n",
    "FROM final_table\n",
    "GROUP BY customer_id, customer_fname, customer_lname\n",
    "ORDER BY TOTAL_AMOUNT_SPENT DESC \n",
    "LIMIT 10\"\"\")\n",
    "\n",
    "display(total_amount_by_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f41a1c6-5de5-4508-94ba-f623b94eb872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. TOTAL ORDER_ITEMS PLACED BY EACH CUSTOMERS:\n",
    "total_orders_per_customer = spark.sql(\"\"\"\n",
    "                                      SELECT customer_id, customer_fname, customer_lname,\n",
    "                                      count(order_item_id) AS total_orders\n",
    "                                      FROM final_table\n",
    "                                      GROUP BY customer_id, customer_fname, customer_lname\n",
    "                                      ORDER BY total_orders DESC\n",
    "                                      \"\"\")\n",
    "display(total_orders_per_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d29867ac-090a-40bb-956f-4e7c71417806",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. write your streaming data into DELTA TABLE\n",
    "###### Use outputMode wsiely based on business use case (append - for new records; update - for updating old records + new incoming records; complete - for rewriting old and old records |==> append doesn't work for aggregating dataframes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e8923c5-1c4f-40a2-813f-4b92feece95e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Streaming_writes = new_1 \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .toTable(\"streaming_delta_table\")\n",
    "    \n",
    "display(spark.sql(\"select * from streaming_delta_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d680029-b299-4114-8706-4229ef482598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from streaming_delta_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157aeb37-2fc8-4326-a66e-6ae44b3f822c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Data modelling \n",
    "##### Once your streaming job done; a finalised model can be modelled in the form of relation modelling or dimentional modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ace3caf5-517a-4d59-9dac-0c1619e65383",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query to create Customer_table_1 DataFrame\n",
    "Customes_details = spark.sql(\"\"\"\n",
    "    SELECT customer_id,\n",
    "           customer_fname, \n",
    "           customer_lname, \n",
    "           city,\n",
    "           state,\n",
    "           pincode\n",
    "    FROM streaming_delta_table\n",
    "\"\"\")\n",
    "\n",
    "# Write the DataFrame to Delta format\n",
    "Customes_details.write.mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"path\", Customer_modelled_data) \\\n",
    "    .saveAsTable(\"customes_details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958f722b-1971-41f4-b559-18e3c3433225",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query from streaming_delta_table\n",
    "order_detail = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        order_id,\n",
    "        customer_id,\n",
    "        order_item_id,\n",
    "        order_item_product_id,\n",
    "        order_item_quantity,\n",
    "        order_item_product_price,\n",
    "        order_item_subtotal\n",
    "    FROM streaming_delta_table\n",
    "\"\"\")\n",
    "\n",
    "# Write the DataFrame as a Delta table\n",
    "order_detail.write.mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"path\", orders_modelled_data) \\\n",
    "    .saveAsTable(\"order_detail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b40f06c-a76e-4754-8150-0bc4f93fc37e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Delta table\n",
    "customer_details_df = spark.read.format(\"delta\").option(\"header\", \"true\").load(Customer_modelled_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(customer_details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e0bca2d-5d8d-429a-b34c-82c604757c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Delta table\n",
    "order_detail_df = spark.read.format(\"delta\").option(\"header\", \"true\").load(orders_modelled_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(order_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c93b8fb-bc3f-4431-82af-9f9953812ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s3_bucket_autoloader_databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
